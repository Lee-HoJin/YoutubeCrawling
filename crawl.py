#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë‚˜ì´í‚¤ YouTube ì±„ë„ ì˜ìƒ ì„¤ëª… ì¶”ì¶œê¸°
YouTube Data API v3 ì‚¬ìš©
"""

import requests
import pandas as pd
import json
from datetime import datetime
import time
import os

class NikeYouTubeScraper:
    def __init__(self, api_key):
        self.api_key = api_key
        self.base_url = 'https://www.googleapis.com/youtube/v3'
        self.nike_channel_handle = '@nike'  # ë‚˜ì´í‚¤ ì±„ë„ í•¸ë“¤
        self.nike_channel_id = None  # ë‚˜ì¤‘ì— ìë™ìœ¼ë¡œ ì°¾ì„ ì˜ˆì •
        
    def get_channel_id_by_handle(self, handle):
        """ì±„ë„ í•¸ë“¤(@nike)ë¡œ ì±„ë„ ID ì°¾ê¸°"""
        # í•¸ë“¤ì—ì„œ @ ì œê±°
        handle = handle.replace('@', '')
        
        # ê²€ìƒ‰ APIë¡œ ì±„ë„ ì°¾ê¸°
        search_url = f"{self.base_url}/search"
        params = {
            'part': 'snippet',
            'q': handle,
            'type': 'channel',
            'key': self.api_key,
            'maxResults': 1
        }
        
        try:
            response = requests.get(search_url, params=params)
            response.raise_for_status()
            data = response.json()
            
            if data['items']:
                channel_id = data['items'][0]['snippet']['channelId']
                channel_title = data['items'][0]['snippet']['title']
                print(f"âœ… ì±„ë„ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤: {channel_title} (ID: {channel_id})")
                return channel_id
            else:
                print("âŒ ì±„ë„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
                
        except requests.exceptions.RequestException as e:
            print(f"âŒ API ìš”ì²­ ì˜¤ë¥˜: {e}")
            return None
    
    def get_channel_uploads_playlist(self, channel_id):
        """ì±„ë„ì˜ ì—…ë¡œë“œ ì¬ìƒëª©ë¡ ID ê°€ì ¸ì˜¤ê¸°"""
        channels_url = f"{self.base_url}/channels"
        params = {
            'part': 'contentDetails',
            'id': channel_id,
            'key': self.api_key
        }
        
        try:
            response = requests.get(channels_url, params=params)
            response.raise_for_status()
            data = response.json()
            
            if data['items']:
                uploads_playlist_id = data['items'][0]['contentDetails']['relatedPlaylists']['uploads']
                print(f"âœ… ì—…ë¡œë“œ ì¬ìƒëª©ë¡ ID: {uploads_playlist_id}")
                return uploads_playlist_id
            else:
                print("âŒ ì—…ë¡œë“œ ì¬ìƒëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
                
        except requests.exceptions.RequestException as e:
            print(f"âŒ ì—…ë¡œë“œ ì¬ìƒëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
            return None
    
    def get_playlist_videos(self, playlist_id, max_results=50):
        """ì¬ìƒëª©ë¡ì˜ ì˜ìƒ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ë” ì•ˆì •ì ì¸ ë°©ë²•)"""
        videos = []
        next_page_token = None
        
        while len(videos) < max_results:
            playlist_url = f"{self.base_url}/playlistItems"
            params = {
                'part': 'snippet',
                'playlistId': playlist_id,
                'maxResults': min(50, max_results - len(videos)),
                'key': self.api_key
            }
            
            if next_page_token:
                params['pageToken'] = next_page_token
            
            try:
                response = requests.get(playlist_url, params=params)
                response.raise_for_status()
                data = response.json()
                
                for item in data['items']:
                    try:
                        # ì¸ë„¤ì¼ URL ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°
                        thumbnail_url = ''
                        thumbnails = item['snippet'].get('thumbnails', {})
                        if 'medium' in thumbnails:
                            thumbnail_url = thumbnails['medium']['url']
                        elif 'default' in thumbnails:
                            thumbnail_url = thumbnails['default']['url']
                        
                        video_info = {
                            'video_id': item['snippet']['resourceId']['videoId'],
                            'title': item['snippet']['title'],
                            'published_at': item['snippet']['publishedAt'],
                            'thumbnail': thumbnail_url
                        }
                        videos.append(video_info)
                        
                    except Exception as e:
                        print(f"âš ï¸  ì˜ìƒ ì •ë³´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        continue
                
                # ë‹¤ìŒ í˜ì´ì§€ í† í° í™•ì¸
                next_page_token = data.get('nextPageToken')
                if not next_page_token:
                    break
                    
                print(f"ğŸ“¹ ì˜ìƒ {len(videos)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ...")
                time.sleep(0.1)  # API ìš”ì²­ ì œí•œ ë°©ì§€
                
            except requests.exceptions.RequestException as e:
                print(f"âŒ ì¬ìƒëª©ë¡ ì˜ìƒ ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
                break
            except Exception as e:
                print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
                break
        
        print(f"âœ… ì´ {len(videos)}ê°œ ì˜ìƒ ëª©ë¡ ìˆ˜ì§‘ ì™„ë£Œ!")
        return videos[:max_results]
    
    def get_channel_videos(self, channel_id, max_results=50):
        """ì±„ë„ì˜ ì˜ìƒ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        videos = []
        next_page_token = None
        
        while len(videos) < max_results:
            search_url = f"{self.base_url}/search"
            params = {
                'part': 'id,snippet',
                'channelId': channel_id,
                'type': 'video',
                'order': 'date',  # ìµœì‹ ìˆœ
                'maxResults': min(50, max_results - len(videos)),
                'key': self.api_key
            }
            
            if next_page_token:
                params['pageToken'] = next_page_token
            
            try:
                response = requests.get(search_url, params=params)
                response.raise_for_status()
                data = response.json()
                
                # ë””ë²„ê¹…: API ì‘ë‹µ êµ¬ì¡° í™•ì¸
                if not videos and data.get('items'):
                    print(f"ğŸ” API ì‘ë‹µ êµ¬ì¡° í™•ì¸: {list(data['items'][0].keys())}")
                    if 'id' in data['items'][0]:
                        print(f"ğŸ” ID êµ¬ì¡°: {data['items'][0]['id']}")
                
                for item in data['items']:
                    try:
                        # videoId ì¶”ì¶œ (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
                        video_id = None
                        if isinstance(item.get('id'), dict):
                            video_id = item['id'].get('videoId')
                        elif isinstance(item.get('id'), str):
                            video_id = item['id']
                        
                        if not video_id:
                            print(f"âš ï¸  ì˜ìƒ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {item.get('id')}")
                            continue
                        
                        # ì¸ë„¤ì¼ URL ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°
                        thumbnail_url = ''
                        thumbnails = item['snippet'].get('thumbnails', {})
                        if 'medium' in thumbnails:
                            thumbnail_url = thumbnails['medium']['url']
                        elif 'default' in thumbnails:
                            thumbnail_url = thumbnails['default']['url']
                        
                        video_info = {
                            'video_id': video_id,
                            'title': item['snippet']['title'],
                            'published_at': item['snippet']['publishedAt'],
                            'thumbnail': thumbnail_url
                        }
                        videos.append(video_info)
                        
                    except Exception as e:
                        print(f"âš ï¸  ì˜ìƒ ì •ë³´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        print(f"   ë¬¸ì œ í•­ëª©: {item}")
                        continue
                
                # ë‹¤ìŒ í˜ì´ì§€ í† í° í™•ì¸
                next_page_token = data.get('nextPageToken')
                if not next_page_token:
                    break
                    
                print(f"ğŸ“¹ ì˜ìƒ {len(videos)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ...")
                time.sleep(0.1)  # API ìš”ì²­ ì œí•œ ë°©ì§€
                
            except requests.exceptions.RequestException as e:
                print(f"âŒ ì˜ìƒ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
                break
            except Exception as e:
                print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
                break
        
        print(f"âœ… ì´ {len(videos)}ê°œ ì˜ìƒ ëª©ë¡ ìˆ˜ì§‘ ì™„ë£Œ!")
        return videos[:max_results]
    
    def get_video_details(self, video_ids):
        """ì˜ìƒ ìƒì„¸ ì •ë³´ (ì„¤ëª… í¬í•¨) ê°€ì ¸ì˜¤ê¸°"""
        videos_details = []
        
        # Noneì´ë‚˜ ë¹ˆ ID ì œê±°
        valid_video_ids = [vid for vid in video_ids if vid and isinstance(vid, str)]
        print(f"ğŸ“Š ìœ íš¨í•œ ì˜ìƒ ID: {len(valid_video_ids)}ê°œ (ì „ì²´: {len(video_ids)}ê°œ)")
        
        if not valid_video_ids:
            print("âŒ ìœ íš¨í•œ ì˜ìƒ IDê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # APIëŠ” í•œë²ˆì— ìµœëŒ€ 50ê°œê¹Œì§€ ì²˜ë¦¬ ê°€ëŠ¥
        for i in range(0, len(valid_video_ids), 50):
            batch_ids = valid_video_ids[i:i+50]
            ids_string = ','.join(batch_ids)
            
            videos_url = f"{self.base_url}/videos"
            params = {
                'part': 'snippet,statistics',
                'id': ids_string,
                'key': self.api_key
            }
            
            try:
                response = requests.get(videos_url, params=params)
                response.raise_for_status()
                data = response.json()
                
                for item in data['items']:
                    try:
                        snippet = item['snippet']
                        stats = item.get('statistics', {})
                        
                        video_detail = {
                            'video_id': item['id'],
                            'title': snippet.get('title', ''),
                            'description': snippet.get('description', ''),
                            'published_at': snippet.get('publishedAt', ''),
                            'channel_title': snippet.get('channelTitle', ''),
                            'tags': ', '.join(snippet.get('tags', [])),
                            'view_count': int(stats.get('viewCount', 0)),
                            'like_count': int(stats.get('likeCount', 0)),
                            'comment_count': int(stats.get('commentCount', 0)),
                            'duration_seconds': 0,  # í•„ìš”ì‹œ ì¶”ê°€ ê°€ëŠ¥
                            'video_url': f"https://www.youtube.com/watch?v={item['id']}"
                        }
                        videos_details.append(video_detail)
                        
                    except Exception as e:
                        print(f"âš ï¸  ì˜ìƒ ìƒì„¸ì •ë³´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        print(f"   ë¬¸ì œ ì˜ìƒ ID: {item.get('id', 'Unknown')}")
                        continue
                
                print(f"ğŸ“ {len(videos_details)}ê°œ ì˜ìƒ ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ...")
                time.sleep(0.1)  # API ìš”ì²­ ì œí•œ ë°©ì§€
                
            except requests.exceptions.RequestException as e:
                print(f"âŒ ì˜ìƒ ìƒì„¸ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
                continue
            except Exception as e:
                print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
                continue
        
        print(f"âœ… ìµœì¢… {len(videos_details)}ê°œ ì˜ìƒ ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ!")
        return videos_details
    
    def extract_nike_descriptions(self, max_videos=100):
        """ë‚˜ì´í‚¤ ì±„ë„ì˜ ì˜ìƒ ì„¤ëª… ì¶”ì¶œ"""
        print("ğŸ” ë‚˜ì´í‚¤ ì±„ë„ ê²€ìƒ‰ ì¤‘...")
        
        # 1. ì±„ë„ ID ì°¾ê¸°
        self.nike_channel_id = self.get_channel_id_by_handle(self.nike_channel_handle)
        if not self.nike_channel_id:
            print("âŒ ë‚˜ì´í‚¤ ì±„ë„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # 2. ì—…ë¡œë“œ ì¬ìƒëª©ë¡ ID ê°€ì ¸ì˜¤ê¸°
        print("ğŸ“‚ ì—…ë¡œë“œ ì¬ìƒëª©ë¡ ì •ë³´ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        uploads_playlist_id = self.get_channel_uploads_playlist(self.nike_channel_id)
        
        if uploads_playlist_id:
            # 3-A. ì¬ìƒëª©ë¡ ë°©ë²•ìœ¼ë¡œ ì˜ìƒ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ë” ì•ˆì •ì )
            print(f"ğŸ“¹ ì¬ìƒëª©ë¡ì—ì„œ ìµœëŒ€ {max_videos}ê°œ ì˜ìƒ ëª©ë¡ ìˆ˜ì§‘ ì¤‘...")
            videos = self.get_playlist_videos(uploads_playlist_id, max_videos)
        else:
            # 3-B. ê²€ìƒ‰ ë°©ë²•ìœ¼ë¡œ í´ë°± (ë°±ì—… ë°©ë²•)
            print(f"ğŸ“¹ ê²€ìƒ‰ ë°©ë²•ìœ¼ë¡œ ìµœëŒ€ {max_videos}ê°œ ì˜ìƒ ëª©ë¡ ìˆ˜ì§‘ ì¤‘...")
            videos = self.get_channel_videos(self.nike_channel_id, max_videos)
        
        if not videos:
            print("âŒ ì˜ìƒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # 4. ì˜ìƒ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        print("ğŸ“ ì˜ìƒ ìƒì„¸ì •ë³´ ë° ì„¤ëª… ìˆ˜ì§‘ ì¤‘...")
        video_ids = [video['video_id'] for video in videos]
        detailed_videos = self.get_video_details(video_ids)
        
        return detailed_videos
    
    def save_to_csv(self, videos_data, filename=None):
        """ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
        if not videos_data:
            print("âŒ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # íŒŒì¼ëª… ìƒì„±
        if not filename:
            # timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"nike_youtube_descriptions.csv"
        
        # DataFrame ìƒì„±
        df = pd.DataFrame(videos_data)
        
        # ë‚ ì§œ í˜•ì‹ ë³€í™˜
        df['published_date'] = pd.to_datetime(df['published_at']).dt.strftime('%Y-%m-%d %H:%M:%S')
        
        # ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬
        columns_order = [
            'video_id', 'title', 'description', 'published_date', 'published_at',
            'channel_title', 'tags', 'view_count', 'like_count', 'comment_count', 'video_url'
        ]
        df = df[columns_order]
        
        # CSV ì €ì¥
        try:
            df.to_csv(filename, index=False, encoding='utf-8-sig')  # í•œê¸€ ì§€ì›
            print(f"âœ… CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")
            print(f"ğŸ“Š ì´ {len(df)}ê°œ ì˜ìƒ ë°ì´í„° ì €ì¥ë¨")
            
            # ê°„ë‹¨í•œ í†µê³„ ì¶œë ¥
            print(f"\nğŸ“ˆ ìˆ˜ì§‘ í†µê³„:")
            print(f"   - í‰ê·  ì¡°íšŒìˆ˜: {df['view_count'].mean():,.0f}íšŒ")
            print(f"   - í‰ê·  ì¢‹ì•„ìš”: {df['like_count'].mean():,.0f}ê°œ")
            print(f"   - í‰ê·  ì„¤ëª… ê¸¸ì´: {df['description'].str.len().mean():.0f}ì")
            print(f"   - ìµœì‹  ì˜ìƒ: {df['published_date'].max()}")
            print(f"   - ê°€ì¥ ì˜¤ë˜ëœ ì˜ìƒ: {df['published_date'].min()}")
            
            return filename
            
        except Exception as e:
            print(f"âŒ CSV ì €ì¥ ì˜¤ë¥˜: {e}")
            return None
    
    def analyze_descriptions(self, videos_data):
        """ì˜ìƒ ì„¤ëª… ê°„ë‹¨ ë¶„ì„"""
        if not videos_data:
            return
        
        df = pd.DataFrame(videos_data)
        
        print(f"\nğŸ” ì˜ìƒ ì„¤ëª… ë¶„ì„:")
        print(f"   - ì´ ì˜ìƒ ìˆ˜: {len(df)}ê°œ")
        print(f"   - ì„¤ëª…ì´ ìˆëŠ” ì˜ìƒ: {len(df[df['description'].str.len() > 0])}ê°œ")
        print(f"   - ë¹ˆ ì„¤ëª…: {len(df[df['description'].str.len() == 0])}ê°œ")
        print(f"   - í‰ê·  ì„¤ëª… ê¸¸ì´: {df['description'].str.len().mean():.0f}ì")
        print(f"   - ìµœì¥ ì„¤ëª… ê¸¸ì´: {df['description'].str.len().max()}ì")
        
        # ìì£¼ ë‚˜ì˜¤ëŠ” í‚¤ì›Œë“œ (ê°„ë‹¨ ë¶„ì„)
        all_descriptions = ' '.join(df['description'].fillna('').astype(str))
        common_words = ['Environment', 'environment', 'Carbon', 'carbon', 'social', 'Social', 'Diversity', 'diversity']
        
        print(f"\nğŸ·ï¸  ì£¼ìš” í‚¤ì›Œë“œ ì¶œí˜„ ë¹ˆë„:")
        for word in common_words:
            count = all_descriptions.count(word)
            if count > 0:
                print(f"   - '{word}': {count}íšŒ")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # âš ï¸ ì—¬ê¸°ì— ë°œê¸‰ë°›ì€ API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”!
    API_KEY = "AIzaSyBpXmp5WoYBM6lVtcegDXAZrzU0hIL2rkA"
    
    # API í‚¤ í™•ì¸
    if API_KEY == "YOUR_YOUTUBE_API_KEY_HERE":
        print("âŒ API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!")
        print("ğŸ“ ì½”ë“œì˜ API_KEY ë³€ìˆ˜ì— ë°œê¸‰ë°›ì€ í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        return
    
    print("ğŸš€ ë‚˜ì´í‚¤ YouTube ì˜ìƒ ì„¤ëª… ì¶”ì¶œ ì‹œì‘!")
    print("=" * 50)
    
    # ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™”
    scraper = NikeYouTubeScraper(API_KEY)
    
    try:
        # ì˜ìƒ ì„¤ëª… ì¶”ì¶œ (ìµœëŒ€ 100ê°œ)
        videos_data = scraper.extract_nike_descriptions(max_videos=300)
        
        if videos_data and len(videos_data) > 0:
            # ê°„ë‹¨ ë¶„ì„
            scraper.analyze_descriptions(videos_data)
            
            # CSV ì €ì¥
            print(f"\nğŸ’¾ CSV íŒŒì¼ë¡œ ì €ì¥ ì¤‘...")
            filename = scraper.save_to_csv(videos_data)
            
            if filename:
                print(f"\nğŸ‰ ì‘ì—… ì™„ë£Œ!")
                print(f"ğŸ“ ì €ì¥ëœ íŒŒì¼: {filename}")
                print(f"ğŸ“Š ì¶”ì¶œëœ ì˜ìƒ ìˆ˜: {len(videos_data)}ê°œ")
                
                # íŒŒì¼ ê²½ë¡œ ì¶œë ¥
                import os
                full_path = os.path.abspath(filename)
                print(f"ğŸ“‚ ì „ì²´ ê²½ë¡œ: {full_path}")
            
        else:
            print("âŒ ë°ì´í„° ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            print("ğŸ’¡ ë‹¤ìŒì„ í™•ì¸í•´ë³´ì„¸ìš”:")
            print("   - API í‚¤ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸")
            print("   - YouTube Data API v3ê°€ í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸")
            print("   - API í• ë‹¹ëŸ‰ì´ ë‚¨ì•„ìˆëŠ”ì§€ í™•ì¸")
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ì‚¬ìš©ìê°€ ì‘ì—…ì„ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        print("\nğŸ”§ ë¬¸ì œ í•´ê²° ë°©ë²•:")
        print("   1. API í‚¤ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸")
        print("   2. ì¸í„°ë„· ì—°ê²° ìƒíƒœ í™•ì¸")
        print("   3. YouTube Data API v3 í• ë‹¹ëŸ‰ í™•ì¸")
        print("   4. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„")
        
        # ë””ë²„ê¹… ì •ë³´
        import traceback
        print(f"\nğŸ› ë””ë²„ê¹… ì •ë³´:")
        traceback.print_exc()

    def test_api_key(self):
        """API í‚¤ê°€ ì‘ë™í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸"""
        print("ğŸ”‘ API í‚¤ í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        # ê°„ë‹¨í•œ API í˜¸ì¶œë¡œ í…ŒìŠ¤íŠ¸
        test_url = f"{self.base_url}/search"
        params = {
            'part': 'snippet',
            'q': 'test',
            'type': 'video',
            'maxResults': 1,
            'key': self.api_key
        }
        
        try:
            response = requests.get(test_url, params=params)
            response.raise_for_status()
            data = response.json()
            
            if 'items' in data:
                print("âœ… API í‚¤ê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤!")
                remaining_quota = response.headers.get('X-RateLimit-Remaining')
                if remaining_quota:
                    print(f"ğŸ“Š ë‚¨ì€ í• ë‹¹ëŸ‰: {remaining_quota}")
                return True
            else:
                print("âŒ API ì‘ë‹µì´ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤.")
                return False
                
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 403:
                print("âŒ API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ YouTube Data API v3ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                print("ğŸ’¡ í•´ê²° ë°©ë²•:")
                print("   1. Google Cloud Consoleì—ì„œ YouTube Data API v3 í™œì„±í™”")
                print("   2. API í‚¤ ì¬ìƒì„±")
                print("   3. í• ë‹¹ëŸ‰ í™•ì¸")
            elif e.response.status_code == 429:
                print("âŒ API í• ë‹¹ëŸ‰ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.")
                print("ğŸ’¡ ë‚´ì¼ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ í• ë‹¹ëŸ‰ì„ ëŠ˜ë ¤ì£¼ì„¸ìš”.")
            else:
                print(f"âŒ HTTP ì˜¤ë¥˜: {e}")
            return False
        except Exception as e:
            print(f"âŒ API í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
            return False

if __name__ == "__main__":
    main()

# ê°„ë‹¨ í…ŒìŠ¤íŠ¸ìš© í•¨ìˆ˜
def test_api_only():
    """API í‚¤ë§Œ ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸"""
    API_KEY = "YOUR_YOUTUBE_API_KEY_HERE"
    
    if API_KEY == "YOUR_YOUTUBE_API_KEY_HERE":
        print("âŒ API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!")
        return
    
    scraper = NikeYouTubeScraper(API_KEY)
    scraper.test_api_key()

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë°©ë²•:
# test_api_only()  # ì£¼ì„ í•´ì œ í›„ ì‹¤í–‰

# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ëª…ë ¹ì–´:
# pip install requests pandas

"""
ì‚¬ìš© ë°©ë²•:
1. API_KEY ë³€ìˆ˜ì— ë°œê¸‰ë°›ì€ YouTube Data API v3 í‚¤ ì…ë ¥
2. í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜:
   pip install requests pandas
3. ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰: python nike_youtube_scraper.py

ê²°ê³¼:
- nike_youtube_descriptions_YYYYMMDD_HHMMSS.csv íŒŒì¼ ìƒì„±
- ì˜ìƒ ID, ì œëª©, ì„¤ëª…, ë°œí–‰ì¼, ì¡°íšŒìˆ˜, ì¢‹ì•„ìš” ìˆ˜ ë“± í¬í•¨
- í•œê¸€ ì„¤ëª…ë„ ì™„ë²½ ì§€ì› (UTF-8)
"""